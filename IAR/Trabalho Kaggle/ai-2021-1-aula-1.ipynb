{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-31T13:52:56.029803Z","iopub.execute_input":"2021-05-31T13:52:56.03037Z","iopub.status.idle":"2021-05-31T13:52:56.039182Z","shell.execute_reply.started":"2021-05-31T13:52:56.030255Z","shell.execute_reply":"2021-05-31T13:52:56.038171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gerando dados para regressão linear simples","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets, linear_model\nimport matplotlib.pyplot as plt\n\nx, y, coef = datasets.make_regression(n_samples=100,#number of samples\n                                      n_features=1,#number of features\n                                      n_informative=1,#number of useful features \n                                      noise=10,#bias and standard deviation of the guassian noise\n                                      coef=True,#true coefficient used to generated the data\n                                      random_state=0) #set for same data points for each run\n\n# Scale feature x (years of experience) to range 0..20\nx = np.interp(x, (x.min(), x.max()), (-20, 20))\n# Scale target y (salary) to range 20000..150000 \ny = np.interp(y, (y.min(), y.max()), (0, 50))\n\nplt.plot(x,y,'.',label='training data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:53:04.241695Z","iopub.execute_input":"2021-05-31T13:53:04.242049Z","iopub.status.idle":"2021-05-31T13:53:05.588204Z","shell.execute_reply.started":"2021-05-31T13:53:04.242016Z","shell.execute_reply":"2021-05-31T13:53:05.587176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelando uma regressão linear","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(x, y)\n\n# Make predictions using the testing set\ny_pred = regr.predict(x)\n\nprint('Coefficients: {}, {}\\n'.format(regr.coef_, regr.intercept_)) # The coefficients\nprint('Mean squared error: %.2f' % mean_squared_error(y, y_pred)) # The mean squared error\nprint('Coefficient of determination: %.2f' % r2_score(y, y_pred)) # The coefficient of determination: 1 is perfect prediction\n\n# Plot outputs\nplt.plot(x, y, '.')\nplt.plot(x, y_pred, color='black', linewidth=1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T13:54:01.549968Z","iopub.execute_input":"2021-05-31T13:54:01.550304Z","iopub.status.idle":"2021-05-31T13:54:01.69948Z","shell.execute_reply.started":"2021-05-31T13:54:01.550277Z","shell.execute_reply":"2021-05-31T13:54:01.698428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelando uma regressão polinomial","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    \"\"\" function to approximate by polynomial interpolation\"\"\"\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\nplt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=2, label=\"ground truth\")\nplt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n\ndegree = 4\nmodel = make_pipeline(PolynomialFeatures(degree), linear_model.Ridge())\n# model = make_pipeline(PolynomialFeatures(degree), linear_model.LinearRegression())\nmodel.fit(X, y)\ny_plot = model.predict(X_plot)\nplt.plot(x_plot, y_plot, color='red', linewidth=2, label=\"degree %d\" % degree)\n\nplt.legend(loc='lower left')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:08:17.602018Z","iopub.execute_input":"2021-05-31T14:08:17.602359Z","iopub.status.idle":"2021-05-31T14:08:17.768065Z","shell.execute_reply.started":"2021-05-31T14:08:17.60233Z","shell.execute_reply":"2021-05-31T14:08:17.767316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.get_params())\nmodel.named_steps['polynomialfeatures'].get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:07:28.289107Z","iopub.execute_input":"2021-05-31T14:07:28.28945Z","iopub.status.idle":"2021-05-31T14:07:28.299161Z","shell.execute_reply.started":"2021-05-31T14:07:28.289421Z","shell.execute_reply":"2021-05-31T14:07:28.298354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model.named_steps['linearregression'].coef_)\n# print(model.named_steps['linearregression'].intercept_)\nprint(model.named_steps['ridge'].coef_)\nprint(model.named_steps['ridge'].intercept_)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:08:34.500041Z","iopub.execute_input":"2021-05-31T14:08:34.500534Z","iopub.status.idle":"2021-05-31T14:08:34.506111Z","shell.execute_reply.started":"2021-05-31T14:08:34.500502Z","shell.execute_reply":"2021-05-31T14:08:34.505167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelando uma Decision Tree","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nx = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(x).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nmax_depth = 100\nregr_1 = DecisionTreeRegressor(max_depth=max_depth)\nregr_1.fit(x, y)\n\n# Predict\nx_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(x_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(x, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\nplt.plot(x_test, y_1, color=\"cornflowerblue\", label=\"max_depth={}\".format(max_depth), linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:16:39.267034Z","iopub.execute_input":"2021-05-31T14:16:39.267339Z","iopub.status.idle":"2021-05-31T14:16:39.426816Z","shell.execute_reply.started":"2021-05-31T14:16:39.267311Z","shell.execute_reply":"2021-05-31T14:16:39.426143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\n_ = tree.plot_tree(regr_1, filled=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:13:29.407816Z","iopub.execute_input":"2021-05-31T14:13:29.408136Z","iopub.status.idle":"2021-05-31T14:13:31.288929Z","shell.execute_reply.started":"2021-05-31T14:13:29.408109Z","shell.execute_reply":"2021-05-31T14:13:31.287982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelando uma Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\nregressor.fit(x, y)\ny_pred = regressor.predict(x)\n\n# Plot the results\nplt.figure()\nplt.scatter(x, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\nplt.plot(x, y_pred, color=\"cornflowerblue\", label=\"RF prediction\", linewidth=2)\n# plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Random Forest Regression\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:21:29.564275Z","iopub.execute_input":"2021-05-31T14:21:29.564628Z","iopub.status.idle":"2021-05-31T14:21:29.743679Z","shell.execute_reply.started":"2021-05-31T14:21:29.564596Z","shell.execute_reply":"2021-05-31T14:21:29.742625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelando uma Logistic Regression","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\n# Cria dados\nX, y = make_classification(200, 2, n_informative=2, n_redundant=0, weights=[.5, .5], random_state=15)\n# Cria regressão logística\nclf = LogisticRegression().fit(X[:100], y[:100])\n\n# Divide os dados em faixas\nxx, yy = np.mgrid[-5:5:.01, -5:5:.01]\ngrid = np.c_[xx.ravel(), yy.ravel()]\n# Calcula a probabilidade entre cada faixa\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\nf, axes = plt.subplots(1, 2, figsize=(16, 6), squeeze=False)\ncontour = axes[0][0].contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)\nax_c = f.colorbar(contour, ax=axes[0][0])\nax_c.set_label(\"$P(y = 1)$\")\nax_c.set_ticks([0, .25, .5, .75, 1])\n\naxes[0][0].scatter(X[100:,0], X[100:, 1], c=y[100:], s=50, cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\", linewidth=1)\naxes[0][0].set(aspect=\"equal\", xlim=(-5, 5), ylim=(-5, 5), xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n\naxes[0][1].contour(xx, yy, probs, levels=[.20], cmap=\"Greys\", vmin=-10, vmax=0)\naxes[0][1].scatter(X[100:,0], X[100:, 1], c=y[100:], s=50, cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\", linewidth=1)\naxes[0][1].set(aspect=\"equal\", xlim=(-5, 5), ylim=(-5, 5), xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T14:36:15.232794Z","iopub.execute_input":"2021-05-31T14:36:15.233145Z","iopub.status.idle":"2021-05-31T14:36:15.998906Z","shell.execute_reply.started":"2021-05-31T14:36:15.233108Z","shell.execute_reply":"2021-05-31T14:36:15.99805Z"},"trusted":true},"execution_count":null,"outputs":[]}]}